---
layout: ../../layouts/BlogPost.astro
title: "Why Counter++ Isn't Thread-Safe: Understanding Atomic Operations"
description: "An explanation about why ++/-- isn't atomic and how it affects concurrent systems!"
date: 2025-12-06
tags: ['programming', 'c', 'go', 'concurrency']
---

While understanding concurrency with multiple threads, I saw an interesting behavior with increment/decrement operations. Consider the following code snippet:

```c
#include <stdio.h>
volatile int counter = 0;
int main(){
	printf("%d",counter++); // still 0
	printf("%d",counter); // 1
	return 0;
}
```

> **Note:** The `volatile` keyword prevents compiler optimizations but doesn't make the operation thread-safe. It ensures each `counter++` actually reads from and writes to memory, but the three-step process (load-increment-store) is still non-atomic at the CPU level.

Programmers are familiar with `++/--` operators which help us increment/decrement values of any number variables by 1. Fairly simple and efficient to utilize.

Let's go a step further and try to run this operation in 2 separate threads where for N input the output should be 2N. (Threads execute multiple instructions in a single process, essentially a process can create N threads to do N processes concurrently [Read more](https://planetscale.com/blog/processes-and-threads#cpu-and-ram)!)

```c
#include "common.h"
#include "common_threads.h"
#include <stdio.h>
#include <stdlib.h>

volatile int counter = 0;
int loops;
void *worker(void *arg) {
  int i;
  for (i = 0; i < loops; i++) {
    counter++;
  }
  return NULL;
}
int main(int argc, char *argv[]) {
  if (argc != 2) {
    fprintf(stderr, "usage: threads <loops>\n");
    exit(1);
  }
  loops = atoi(argv[1]);
  pthread_t p1, p2;
  printf("Initial value : %d\n", counter);
  Pthread_create(&p1, NULL, worker, NULL);
  Pthread_create(&p2, NULL, worker, NULL);
  Pthread_join(p1, NULL);
  Pthread_join(p2, NULL);
  printf("Final value   : %d\n", counter);
  return 0;
}
```

You would predict that due to 2 threads simultaneously updating the same variable, output should be deterministic i.e. for input `1000` the output is `2000`, which is exactly what we observe here.

```bash
❯ ./threads 1000
Initial value : 0
Final value   : 2000
```

But let's try with a higher value

```bash
❯ ./threads 10000
Initial value : 0
Final value   : 14900
```

Um wait a second? Shouldn't it be 20000? Let's run this one more time maybe it will get fixed (obviously it's not):

```bash
❯ ./threads 10000
Initial value : 0
Final value   : 14026
```

Woah! Not only are we not seeing the desired 2N output but we also see completely different outputs at each execution of the program!

This is due to the **Non-Atomic** nature of increment/decrement operations.

**Atomicity**:
Is a property of an operation or a set of operations that guarantees to be executed in a single, divisible unit.
In the case of inc/dec operations, let's try to break down what exactly is supposed to happen when it's executed.

1. First you would need to read the value(Load)
2. Increment the existing value by 1(Increment)
3. Store the newly updated value in the same memory location!(Store)

`Load -> Increment -> Store`

These are 3 individual operations happening!
Let's confirm this by seeing the machine instructions generated by the program. This will generate the assembly code for your CPU Architecture (this will be different for each machine but the logic would be the same).

```bash
#Using the -S flag we can generate the asm code
gcc -S -o threads threads.c
```

```asm
mov counter(%rip), %eax
add $1, %eax
mov %eax, counter(%rip)
```

Decoding each instruction:

1. LOAD:
   Read memory location 'counter' into register eax
   counter(%rip) means "address of counter relative to instruction pointer"
   Takes ~4 CPU cycles (if in L1 cache)
2. INCREMENT:
   Add immediate value 1 to register eax
   Takes ~1 CPU cycle
   ONLY the register changes, memory is still old value!
3. STORE:
   Write register eax back to memory location 'counter'
   Takes ~4 CPU cycles (if in L1 cache)

To better visualize:

```
Time →
Thread 1: [Read=5]     [Inc=6]   [Write=6]
Thread 2:       [Read=5][Inc=6]        [Write=6]
                    ↑ Both read 5!              ↑ Both write 6!
Result: Lost one increment! Counter is 6 instead of 7
```

The two threads due to fast concurrent context switching sometimes overlap and increment both values at the same time!
This is known as a `Race Condition`. When N threads/goroutines (in golang) try updating the same memory location this results in `undefined behavior`.

To tackle this challenge programming languages have their own solutions,
In C you can use the built-in `stdatomic.h` header and it's `atomic_fetch_add` function to perform operation in a SINGLE CPU CYCLE!.

```c
#include <stdatomic.h>

atomic_int counter = 0;
atomic_fetch_add(&counter, 1);
```

In golang you can either utilize mutexes which lock the value (blocks usage of other threads or lets them wait) then unlocks it for other threads to utilize.
OR
you can use the `sync/atomic` package's `atomic.AddInt64()` function to perform atomic operations.

```go
import "sync/atomic"
var counter int32
atomic.AddInt32(&counter, 1) // Add
atomic.LoadInt32(&counter) // Load
atomic.StoreInt32(&counter, 42) // Store
```

The machine code for atomic operation looks like this!

```asm
# Atomic increment uses special CPU instruction:
LOCK INCQ 0x618dd8    # Single atomic instruction

# Mutex uses multiple instructions + OS calls:
# 1. Try to acquire lock (might block, context switch)
# 2. Increment
# 3. Release lock (wake up waiting threads)
```

When you are dealing with distributed and fast backend systems concurrency is pretty much present everywhere for more performance.
It's nice to understand why are things made the way they have been made and going one layer deep always helps!

That's it, hope you understood!

# Note:

For the C example the go code would utilize go routines:

```go
package main
import (
	"fmt"
	"os"
	"strconv"
	"sync"
)
var counter int
var loops int
func worker(wg *sync.WaitGroup) {
	defer wg.Done()
	for i := 0; i < loops; i++ {
		counter++
	}
}
func main() {
	if len(os.Args) != 2 {
		fmt.Fprintf(os.Stderr, "Usage: threads <loops>\n")
		os.Exit(1)
	}
	var err error
	loops, err = strconv.Atoi(os.Args[1])
	if err != nil {
		os.Exit(1)
	}
	var wg sync.WaitGroup
	fmt.Printf("Initial counter value: %d\n", counter)
	wg.Add(2)
	go worker(&wg)
	go worker(&wg)
	wg.Wait()
	fmt.Printf("Final counter value: %d\n", counter)

}
```

If you run the file with -race flag you can easily see where the race condition is happening:

```bash
❯ go run -race main.go 100000
Initial counter value: 0
==================
WARNING: DATA RACE
Read at 0x000000618dd8 by goroutine 9:
  main.worker()
      /home/fyzanshaik/workspace/os/ostep-code/intro/main.go:16 +0x88
  main.main.gowrap2()
      /home/fyzanshaik/workspace/os/ostep-code/intro/main.go:37 +0x33
```

To visualize the obj dump use the go tool chain:

```bash
go tool objdump main.go > tmp.txt #Stores it inside a temporary file
```

# References

[Arpit Bhayani's: Why count++ is not atomic](https://www.youtube.com/watch?v=kBHd7kn_1EU&t=703s)

[Process vs threads planetscale blog](https://planetscale.com/blog/processes-and-threads#cpu-and-ram)
